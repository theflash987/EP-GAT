{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 220043,
     "status": "ok",
     "timestamp": 1757060194610,
     "user": {
      "displayName": "东d",
      "userId": "06469327374915729739"
     },
     "user_tz": -60
    },
    "id": "59W5FwiND0-U",
    "outputId": "a94a0d9c-4599-445c-c3f3-3ebbb76127cd"
   },
   "outputs": [],
   "source": [
    "!pip install torch===2.3.1\n",
    "import torch\n",
    "\n",
    "def format_pytorch_version(version):\n",
    "  return version.split('+')[0]\n",
    "\n",
    "TORCH_version = torch.__version__\n",
    "TORCH = format_pytorch_version(TORCH_version)\n",
    "\n",
    "def format_cuda_version(version):\n",
    "  return 'cu' + version.replace('.', '')\n",
    "\n",
    "CUDA_version = torch.version.cuda\n",
    "CUDA = format_cuda_version(CUDA_version)\n",
    "\n",
    "!pip install torch-scatter -f https://data.pyg.org/whl/torch-2.3.1+{CUDA}.html\n",
    "!pip install torch-sparse -f https://data.pyg.org/whl/torch-2.3.1+{CUDA}.html\n",
    "!pip install torch-cluster -f https://data.pyg.org/whl/torch-2.3.1+{CUDA}.html\n",
    "!pip install torch-spline-conv -f https://data.pyg.org/whl/torch-2.3.1+{CUDA}.html\n",
    "!pip install torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IzeerMPJXsvf"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Dataset Defintion and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y16OomKxD73c"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Tuple\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.csgraph import laplacian\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, root: str, desti: str, market: str, comlist: List[str], start: str, end: str, window: int, dataset_type: str, k: float, threshold: float):\n",
    "        super().__init__()\n",
    "\n",
    "        self.comlist = comlist\n",
    "        self.market = market\n",
    "        self.root = root\n",
    "        self.desti = desti\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.window = window\n",
    "        self.dates, self.next_day = self.find_dates(start, end, root, comlist, market)\n",
    "        self.dataset_type = dataset_type\n",
    "        self.k = k\n",
    "        self.threshold = threshold\n",
    "        graph_files_exist = all(os.path.exists(os.path.join(desti, f'{market}_{dataset_type}_{start}_{end}_{window}_{k}_{threshold}/graph_{i}.pt')) for i in range(len(self.dates) - window + 1))\n",
    "        if not graph_files_exist:\n",
    "            self._create_graphs(self.dates, desti, comlist, market, root, window)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dates) - self.window + 1\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        directory_path = os.path.join(self.desti, f'{self.market}_{self.dataset_type}_{self.start}_{self.end}_{self.window}_{self.k}_{self.threshold}')\n",
    "        data_path = os.path.join(directory_path, f'graph_{idx}.pt')\n",
    "        if os.path.exists(data_path):\n",
    "            return torch.load(data_path)\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"No graph data found for index {idx}\")\n",
    "\n",
    "    def check_years(self, date_str: str, start_str: str, end_str: str) -> bool:\n",
    "        date_format = \"%Y-%m-%d\"\n",
    "        # strptime() can turn a str to a datetime object, which allows the comparsions among datetime objects\n",
    "        date = datetime.strptime(date_str, date_format)\n",
    "        start = datetime.strptime(start_str, date_format)\n",
    "        end = datetime.strptime(end_str, date_format)\n",
    "        return start <= date <= end\n",
    "\n",
    "    def find_dates(self, start: str, end: str, path: str, comlist: List[str], market: str) -> Tuple[List[str], str]:\n",
    "        # Find the common date\n",
    "        date_sets = []\n",
    "        after_end_date_sets = []\n",
    "\n",
    "        for h in comlist:\n",
    "            dates = set()\n",
    "            after_end_dates = set()\n",
    "            d_path = os.path.join(path,market, f'{h}.csv')\n",
    "\n",
    "            with open(d_path) as f:\n",
    "              file = csv.reader(f)\n",
    "              next(file, None)  # Skip the header row\n",
    "              for line in file:\n",
    "                  date_str = line[0][:10]\n",
    "\n",
    "                  if self.check_years(date_str, start, end):\n",
    "                      dates.add(date_str)\n",
    "                  elif self.check_years(date_str, end, '2024-07-11'):\n",
    "                      after_end_dates.add(date_str)\n",
    "\n",
    "            date_sets.append(dates)\n",
    "            after_end_date_sets.append(after_end_dates)\n",
    "\n",
    "        all_dates = list(set.intersection(*date_sets))\n",
    "        all_after_end_dates = list(set.intersection(*after_end_date_sets))\n",
    "        next_common_day = min(all_after_end_dates) if all_after_end_dates else None\n",
    "\n",
    "        return sorted(all_dates), next_common_day\n",
    "\n",
    "    def signal_energy(self, x_tuple: Tuple[float]) -> float:\n",
    "        x = np.array(x_tuple)\n",
    "        return np.sum(np.square(x))\n",
    "\n",
    "    def adjacency(self, X: torch.Tensor) -> torch.sparse_coo_tensor:\n",
    "        energies = torch.tensor([self.signal_energy(tuple(x)) for x in X])\n",
    "        A = torch.zeros((X.shape[0], X.shape[0]))\n",
    "        k = self.k\n",
    "\n",
    "        for i in range(X.shape[0]):\n",
    "            energy_diffs = torch.exp(-torch.abs(energies[i] - energies)/(k * self.window))\n",
    "\n",
    "            energy_sum = energy_diffs.sum()\n",
    "\n",
    "            # Probability of connection i -> j\n",
    "            A[i] = energy_diffs / energy_sum\n",
    "\n",
    "        A[A < self.threshold] = 0\n",
    "\n",
    "        row_indices, col_indices = A.nonzero(as_tuple=True)\n",
    "        values = A[row_indices, col_indices]\n",
    "\n",
    "        A_coo = torch.sparse_coo_tensor(torch.stack([row_indices, col_indices]), values, A.size())\n",
    "\n",
    "        return A_coo\n",
    "\n",
    "    def random(self, X: torch.Tensor) -> torch.sparse_coo_tensor:\n",
    "        num_nodes = X.shape[0]\n",
    "        A = torch.rand((num_nodes, num_nodes))\n",
    "\n",
    "        A[A < 0.5] = 0\n",
    "\n",
    "        row_indices, col_indices = A.nonzero(as_tuple=True)\n",
    "        values = A[row_indices, col_indices]\n",
    "        A_coo = torch.sparse_coo_tensor(torch.stack([row_indices, col_indices]), values, A.size())\n",
    "\n",
    "        return A_coo\n",
    "        \n",
    "    def node_feature_matrix(self, dates: List[str], comlist: List[str], market: str, path: str) -> torch.Tensor:\n",
    "        # [7, len(comlist), len(dates)]\n",
    "        # 7 features: 4 log price features + 3 derived features (oc, gap, hl_range)\n",
    "        dates_dt = pd.to_datetime([d for d in dates if d is not None])\n",
    "        X = torch.zeros((7, len(comlist), len(dates_dt)), dtype=torch.float32)\n",
    "\n",
    "        eps = 1e-8 \n",
    "        for idx, h in enumerate(comlist):\n",
    "            d_path = os.path.join(path, market, f'{h}.csv')\n",
    "            df = pd.read_csv(d_path, parse_dates=['Date'], index_col='Date').reindex(dates_dt)\n",
    "\n",
    "            # log price\n",
    "            logC = np.log(df['Adj Close'].astype(float).clip(lower=eps))\n",
    "            logH = np.log(df['High'].astype(float).clip(lower=eps))\n",
    "            logL = np.log(df['Low'].astype(float).clip(lower=eps))\n",
    "            logO = np.log(df['Open'].astype(float).clip(lower=eps))\n",
    "\n",
    "            r_close = logC.diff().fillna(0.0)  \n",
    "            r_high = logH.diff().fillna(0.0)  \n",
    "            r_low = logL.diff().fillna(0.0)   \n",
    "            r_open = logO.diff().fillna(0.0)\n",
    "\n",
    "            # Open-to-Close Return\n",
    "            # Overnight Gap\n",
    "            # High-Low Range\n",
    "            oc = (logC - logO)                        \n",
    "            gap = (logO - logC.shift(1)).fillna(0.0)  \n",
    "            hl_range = (logH - logL).clip(lower=0.0)      \n",
    "\n",
    "            X[:, idx, :] = torch.from_numpy(\n",
    "                np.vstack([\n",
    "                    r_close.values, r_high.values, r_low.values, r_open.values,\n",
    "                    oc.values, gap.values, hl_range.values\n",
    "                ])\n",
    "            ).to(torch.float32)\n",
    "\n",
    "        return X\n",
    "\n",
    "    def z_score(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return (x - x.mean()) / x.std()\n",
    "\n",
    "    def _create_graphs(self, dates: List[str], desti: str, comlist: List[str], market: str, root: str, window: int):\n",
    "        dates.append(self.next_day)\n",
    "\n",
    "        for i in tqdm(range(min(len(dates) - window, 120))):\n",
    "            directory_path = os.path.join(desti, f'{market}_{self.dataset_type}_{self.start}_{self.end}_{window}_{self.k}_{self.threshold}')\n",
    "            filename = os.path.join(directory_path, f'graph_{i}.pt')\n",
    "\n",
    "            if os.path.exists(filename):\n",
    "                print(f\"Graph {i}/{len(dates) - window + 1} already exists, skipping...\")\n",
    "                continue\n",
    "\n",
    "            print(f'Generating graph {i}/{len(dates) - window + 1}...')\n",
    "\n",
    "            box = dates[i:i + window + 1]\n",
    "            X = self.node_feature_matrix(box, comlist, market, root)\n",
    "            C = (X[0, :, -1] > 0).to(torch.long)\n",
    "\n",
    "            X = X[:,:,:-1]\n",
    "            X = X.permute(1, 2, 0)\n",
    "            X = X.reshape(X.shape[0], -1)\n",
    "            X = self.z_score(X)\n",
    "            A = self.adjacency(X)\n",
    "\n",
    "            os.makedirs(directory_path, exist_ok=True)\n",
    "\n",
    "            torch.save({'X': X, 'A': A, 'Y': C}, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Graph Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 18205,
     "status": "ok",
     "timestamp": 1757060214585,
     "user": {
      "displayName": "东d",
      "userId": "06469327374915729739"
     },
     "user_tz": -60
    },
    "id": "KjwBPGwWEZ-D",
    "outputId": "8b670541-2ba4-4f94-f00f-0cdc88a2e298"
   },
   "outputs": [],
   "source": [
    "root = 'PATH TO YOUR DATA DIRECTORY'\n",
    "desti = 'PATH TO YOUR OUTPUT DIRECTORY'\n",
    "\n",
    "markets = ['LSE'] # Market options: 'FTSE', 'LSE', 'NASDAQ', 'NYSE', 'SP'\n",
    "start_train = '2021-07-12'\n",
    "end_train = '2023-07-11'\n",
    "start_test = '2023-07-12'\n",
    "end_test = '2024-07-11'\n",
    "dataset_type = 'train'\n",
    "dataset_trains = []\n",
    "dataset_tests = []\n",
    "\n",
    "# Hyperparameters\n",
    "k_value = 0.08\n",
    "window_size = 14\n",
    "s = 0.55\n",
    "\n",
    "for market in markets:\n",
    "  comlist = []\n",
    "  folder_path = f'{root}/{market}'\n",
    "  for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "      company_name = filename[:-4]\n",
    "      comlist.append(company_name)\n",
    "  comlist = list(dict.fromkeys(comlist))\n",
    "  dataset_train = MyDataset(root, desti, market, comlist, start_train, end_train, window_size, dataset_type, k_value, s)\n",
    "  dataset_trains.append(dataset_train)\n",
    "\n",
    "  dataset_test = MyDataset(root, desti, market, comlist, start_test, end_test, window_size, 'test', k_value, s)\n",
    "  dataset_tests.append(dataset_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5B9unyvdE2JJ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from torch_geometric.data import Data\n",
    "import torch_geometric.transforms as T\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class GATABlock(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, class_dim, num_heads, time_stamp, window, is_first_block=False, is_concat=True):\n",
    "        super().__init__()\n",
    "        self.conv = GATv2Conv(in_dim, out_dim // num_heads, heads=num_heads, concat=True)\n",
    "        self.time_stamp = time_stamp\n",
    "        self.transfer_layer = nn.Linear(out_dim, time_stamp)\n",
    "        self.is_first_block = is_first_block\n",
    "        self.is_concat = is_concat\n",
    "        self.window = window\n",
    "        self.attention = nn.MultiheadAttention(out_dim // window, num_heads, batch_first=True)\n",
    "\n",
    "        if not is_first_block:\n",
    "            self.skip_layer = nn.Linear(in_dim, out_dim)\n",
    "        else:\n",
    "            self.activation = nn.PReLU()\n",
    "            self.activation_att = nn.PReLU()\n",
    "\n",
    "        self.temp_linear = nn.Linear(out_dim + time_stamp, out_dim) if is_concat else nn.Identity()\n",
    "\n",
    "    def forward(self, x, edge_index, h_prime):\n",
    "        h = self.conv(x, edge_index)\n",
    "\n",
    "        if not self.is_first_block:\n",
    "            h += self.skip_layer(x)\n",
    "        else:\n",
    "            h = self.activation(h)\n",
    "\n",
    "        h_shape = h.shape[1]\n",
    "        if self.is_concat and h_prime is not None:\n",
    "            h_att = torch.cat((h_prime, h), dim=1)\n",
    "            h_att = self.temp_linear(h_att)\n",
    "        else:\n",
    "            h_att = h\n",
    "\n",
    "        h_att = h_att.view(x.size(0), self.window, -1)\n",
    "        h_att, w = self.attention(h_att, h_att, h_att)\n",
    "        if self.is_first_block:\n",
    "            h_att = self.activation_att(h_att)\n",
    "        h_att = h_att.reshape(x.size(0), -1)\n",
    "        h_att = self.transfer_layer(h_att)\n",
    "\n",
    "        return h, h_att, w\n",
    "\n",
    "class GATA(nn.Module):\n",
    "    def __init__(self, num_blocks, dim_list, class_dim, num_heads, window, time_stamp, is_concat=True):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList()\n",
    "        self.num_blocks = num_blocks\n",
    "        self.time_stamp = time_stamp\n",
    "        self.window = window\n",
    "        self.class_dim = class_dim\n",
    "        self.final_layer = nn.Linear(time_stamp, class_dim)\n",
    "        self.is_concat = is_concat\n",
    "\n",
    "        for i in range(num_blocks):\n",
    "            is_first_block = (i == 0)\n",
    "            self.blocks.append(GATABlock(dim_list[i], dim_list[i+1], class_dim, num_heads, time_stamp, window, is_first_block, is_concat))\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data['X'], data['A']\n",
    "        h_list = []\n",
    "        w_list = []\n",
    "        h_prime = None\n",
    "\n",
    "        h = x\n",
    "        for block in self.blocks:\n",
    "            h, h_att, w = block(h, edge_index, h_prime)\n",
    "            h_list.append(h_att)\n",
    "            w_list.append(w)\n",
    "            h_prime = h_att\n",
    "\n",
    "        h_final = torch.sum(torch.stack([h for h in h_list]), dim=0)\n",
    "        h_final = self.final_layer(h_final)\n",
    "\n",
    "        return h_final, w_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 1526630,
     "status": "ok",
     "timestamp": 1757066201226,
     "user": {
      "displayName": "东d",
      "userId": "06469327374915729739"
     },
     "user_tz": -60
    },
    "id": "45maTDv1E9og",
    "outputId": "90531785-965c-46fd-b5e3-586201c130e0"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, matthews_corrcoef, f1_score\n",
    "dim_head = [1, 18, 12, 6]\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "attention_maps = {}\n",
    "\n",
    "for j in range(1):\n",
    "  dataset_train = dataset_trains[j]\n",
    "  dataset_test = dataset_tests[j]\n",
    "  time_stamp = dataset_train[0]['X'].shape[1]\n",
    "  window = time_stamp // 7\n",
    "  dim_list = [time_stamp*rate for rate in dim_head]\n",
    "  # GATA(num_blocks, dim_list, class_dim, num_heads, window, time_stamp)\n",
    "  model = GATA(3, dim_list, 2, 14, window, time_stamp, True).to(device)\n",
    "  for i in range(1):\n",
    "      dir = 'PATH TO YOUR LOG DIRECTORY'\n",
    "      os.makedirs(dir, exist_ok=True)\n",
    "      log_path = os.path.join(dir, f'training_attention_map.txt')\n",
    "      log_file = open(log_path, 'w')\n",
    "      optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=6e-4)\n",
    "      scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=500, eta_min=1e-5)\n",
    "      min_loss = 1\n",
    "      max_acc = 0\n",
    "      max_test = 0\n",
    "      all_true_labels = []\n",
    "      all_pred_labels = []\n",
    "      for epoch in range(500):\n",
    "\n",
    "          model.train()\n",
    "          objective = 0\n",
    "          optimizer.zero_grad()\n",
    "\n",
    "          for idx, i in enumerate(dataset_train):\n",
    "            if idx > 99:\n",
    "              break\n",
    "            else:\n",
    "                data = {k: v.to(device) for k, v in i.items()}\n",
    "                data['Y'] = data['Y'].to(torch.long)\n",
    "                out, _ = model(data)\n",
    "                loss = F.cross_entropy(out, data['Y'])\n",
    "                objective += loss\n",
    "\n",
    "          objective = objective / 100\n",
    "          objective.backward()\n",
    "          optimizer.step()\n",
    "          scheduler.step()\n",
    "\n",
    "          model.eval()\n",
    "          acc_t = 0.0\n",
    "          test_a = 0.0\n",
    "          true_labels = []\n",
    "          pred_labels = []\n",
    "          with torch.no_grad():\n",
    "            for idx, i in enumerate(dataset_train):\n",
    "              if idx > 99:\n",
    "                break\n",
    "              else:\n",
    "                data = {k: v.to(device) for k, v in i.items()}\n",
    "                data['Y'] = data['Y'].to(torch.long)\n",
    "                pred, _ = model(data)\n",
    "                pred = pred.argmax(dim=1)\n",
    "                correct = (pred == data['Y']).sum().item()\n",
    "                acc_t += int(correct) / int(data['Y'].shape[0])\n",
    "\n",
    "          model.eval()\n",
    "          with torch.no_grad():\n",
    "            for idx, i in enumerate(dataset_test):\n",
    "              if idx > 19:\n",
    "                break\n",
    "              else:\n",
    "                data_test = {k: v.to(device) for k, v in i.items()}\n",
    "                data_test['Y'] = data_test['Y'].to(torch.long)\n",
    "                pred_test, w_list = model(data_test)\n",
    "                pred_test = pred_test.argmax(dim=1)\n",
    "                correct_pred = (pred_test == data_test['Y']).sum().item()\n",
    "                test_a += correct_pred / data_test['Y'].shape[0]\n",
    "                true_labels.extend(data_test['Y'].cpu().numpy())\n",
    "                pred_labels.extend(pred_test.cpu().numpy())\n",
    "\n",
    "\n",
    "          avg_test = test_a / 20\n",
    "          avg_acc = acc_t / 100\n",
    "          min_loss = objective if objective < min_loss else min_loss\n",
    "          max_acc = avg_acc if avg_acc > max_acc else max_acc\n",
    "          max_test = avg_test if avg_test > max_test else max_test\n",
    "          all_true_labels.extend(true_labels)\n",
    "          all_pred_labels.extend(pred_labels)\n",
    "\n",
    "          if epoch == 0 or epoch == 399:\n",
    "              attention_maps[epoch] = w_list\n",
    "\n",
    "          log_file.write(f'Epoch {epoch+1}: Accuracy={avg_acc:.4f}, Loss={objective.item():.4f}, Test={avg_test:.4f}\\n')\n",
    "          print(f'Epoch {epoch+1}: Accuracy={avg_acc:.4f}, Loss={objective.item():.4f}, Test={avg_test:.4f}, Max={max_test:.4f}')\n",
    "\n",
    "      final_acc_test = accuracy_score(all_true_labels, all_pred_labels) * 100\n",
    "      final_mcc_test = matthews_corrcoef(all_true_labels, all_pred_labels)\n",
    "      final_f1_test = f1_score(all_true_labels, all_pred_labels, average='macro')\n",
    "\n",
    "      log_file.write(f'Final Test Results: ACC={final_acc_test:.2f}%, MCC={final_mcc_test:.4f}, F1-Score={final_f1_test:.4f}\\n')\n",
    "      print(f'Final Test Results: ACC={final_acc_test:.2f}%, MCC={final_mcc_test:.4f}, F1-Score={final_f1_test:.4f}')\n",
    "\n",
    "      print(f'Max Accuracy={max_acc:.4f}, Min Loss={min_loss:.4f}, Max Test={max_test:.4f}')\n",
    "      log_file.write(f'Max Accuracy={max_acc:.4f}, Min Loss={min_loss:.4f}, Max Test={max_test:.4f}\\n')\n",
    "      log_file.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
